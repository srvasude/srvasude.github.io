[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Trapezoid Rule Monte Carlo\n\n\n\nmonte-carlo\n\nquadrature\n\n\n\nMixing the Trapezoid Rule with Monte Carlo\n\n\n\nSrinivas Vasudevan\n\n\nSep 19, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Srinivas Vasudevan",
    "section": "",
    "text": "Formerly at Google Research.\nBelow is my list of publications. See my Blog for recent writing or Board Games for boardgame design."
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Srinivas Vasudevan",
    "section": "",
    "text": "Formerly at Google Research.\nBelow is my list of publications. See my Blog for recent writing or Board Games for boardgame design."
  },
  {
    "objectID": "index.html#packages",
    "href": "index.html#packages",
    "title": "Srinivas Vasudevan",
    "section": "Packages",
    "text": "Packages\nI’ve worked on libraries in the general ML infrastructure, with a focus on probability and scientific computing\n Eigen,  JAX,  TensorFlow Probability,  TensorFlow"
  },
  {
    "objectID": "posts/quad_mc.html",
    "href": "posts/quad_mc.html",
    "title": "Trapezoid Rule Monte Carlo",
    "section": "",
    "text": "I recently encountered a Monte Carlo estimator (Masry and Cambanis 1990) that uses the trapezoid rule and wanted to write out / generalize it.\nGiven a function \\(f\\) (we’ll assume \\(f\\) is \\(C^\\infty\\) for simplicity, but some of these arguments can be changed depending on the smoothness) defined on the unit interval \\([0, 1]\\), we would like to compute the integral \\(I = \\int_0^1 f(x) dx\\). Quadrature methods can easily deal with estimating these integrals 1\nOur plan will be to come up with a lower variance (but biased) estimator, and then generalize to computing integrals of the form \\(\\int_A f(x) p(x) dx\\), where \\(p(x)\\) is a probability density on some subset of \\(\\mathbb{R}\\). Finally, we’ll be able to get an estimator even if we only have access to a sampler and the unnormalized log density (e.g. in the case of MCMC).\nThe typical Monte-Carlo estimator for this type of integral is \\(S_n = \\frac{1}{n}\\sum_{i=1}^n f(u_i)\\), where \\(u_i \\sim U(0, 1)\\) are IID uniform samples. This is an unbiased estimator for \\(I\\) and has variance \\(var(S_n) = \\frac{var(f)}{n}\\), where \\(var(f)\\) is the variance of the random variable \\(f(U), U \\sim U(0, 1)\\). In particular the variance is inversely proportional to \\(n\\)."
  },
  {
    "objectID": "posts/quad_mc.html#trapezoid-rule-estimator",
    "href": "posts/quad_mc.html#trapezoid-rule-estimator",
    "title": "Trapezoid Rule Monte Carlo",
    "section": "Trapezoid Rule Estimator",
    "text": "Trapezoid Rule Estimator\nIf we look at error estimates of various quadrature rules, we see that the errors get better dependency than \\(O(\\frac{1}{\\sqrt n})\\) for estimating \\(I\\).\n\n\nWe can see from the table that we can get errors \\(S_n - I\\) that are at least \\(O(\\frac{1}{n^2})\\) for various Newton-Cotes formula. From wikipedia.\n\n\n\n\n\n\n\nName\nError Bound\n\n\n\n\nTrapezoid Rule\n\\(-\\frac{1}{12n^2}f''(\\xi)\\)\n\n\nSimpson’s \\(\\frac{1}{3}\\) rule Rule\n\\(-\\frac{1}{90n^4}f^{(4)}(\\xi)\\)\n\n\nBoole’s Rule\n\\(-\\frac{8}{945n^6}f^{(6)}(\\xi)\\)\n\n\n\nwhere \\(\\xi\\) lies in the interval of integration \\([a, b]\\).\nThe main idea is we can exploit the structure of the trapezoid rule (and in general Newton-Cotes rules) to get better scaling of the variance with respect to the number of samples \\(n\\).\nGiven \\(n\\) samples \\(u_i \\sim U(0, 1)\\), we can sort the samples to form the order statistics \\(v_i\\). We’ll also add the points \\(0\\) and \\(1\\) to get \\(n + 1\\) points (let \\(v_0 = 0\\) and \\(v_{n + 1} = 1\\)).\nWe can then consider the trapezoid rule estimator: \\(S'_n = \\sum_{i=0}^{n} \\frac{1}{2} (v_{i+1} - v_i)(f(v_{i+1}) + f(v_i))\\)\n\n\n\n\n\n\n\n\n\nFigure 1: Randomized Trapezoid Rule for \\(\\sin(\\frac{\\pi}{2} x)\\)\n\n\n\n\nThe intuition is that for sufficiently smooth \\(f\\), \\(S'_n\\) should be a good estimator of the integral \\(I\\) due to the use of the trapezoid rule. Note that \\(S'_n\\) may be biased, and indeed Figure 1 shows an example of this, where the trapezoid rule approximates the integral well, but will always be an underestimate due to the concavity of the integrand. The hope is that we may trade off some bias for a reduction in variance.\nAs mentioned above, we can note the following deficiencies of this method, compared to quadrature rules and monte carlo rules.\n\nThis can be a biased estimator.\nThis is for \\(U(0, 1)\\) random variables and in one dimension, for which there are better methods.\nIn the current formulation, you have to evaluate the endpoints \\(0\\) and \\(1\\). This can be problematic for integrands (such as certain \\(Beta(\\alpha, \\beta)\\) densities which go off to \\(\\infty\\) in the tails). We can actually get away with \\(n\\) points without evaluating the \\(0\\) and \\(1\\) points. Adding the points just simplifies the analysis and reduces the error by a constant."
  },
  {
    "objectID": "posts/quad_mc.html#error-bound-on-s_n",
    "href": "posts/quad_mc.html#error-bound-on-s_n",
    "title": "Trapezoid Rule Monte Carlo",
    "section": "Error bound on \\(S'_n\\)",
    "text": "Error bound on \\(S'_n\\)\nLet’s take a look at the error \\(S'_n - I\\).\nFor simplicity we’ll reuse a form of the Trapezoid Rule Error 2.\n2 See hereIf we are estimating \\(\\int_{v_i}^{v_{i+1}} f(x) dx\\), then the error of using \\(\\frac{1}{2}(f(v_{i+1}) + f(v_{i}))(v_{i+1} - v_i)\\) is bounded by \\(\\frac{K_i(v_{i+1} - v_i)^3}{12}\\), where \\(K_i = \\sup_{[v_i, v_{i+1}]} |f''(x)|\\) (this can be derived through repeated applications of the Mean Value Theorem).\n\\(|S'_n - I| = |-\\int_0^1 f(x) dx + \\sum_{i=0}^{n}\\frac{1}{2}(v_{i+1} - v_i) (f(v_{i+1}) - f(v_i))|\\)\n\\(=|\\sum_{i=0}^{n}\\frac{1}{2}(v_{i+1} - v_i) (f(v_{i+1}) + f(v_i)) - \\int_{v_i}^{v_{i+1}} f(x) dx|\\)\n\\(\\le\\sum_{i=0}^{n}|\\frac{1}{2}(v_{i+1} - v_i) (f(v_{i+1}) + f(v_i)) - \\int_{v_i}^{v_{i+1}} f(x) dx|\\)\n\\(\\le\\sum_{i=0}^{n}\\frac{K_i}{12}(v_{i + 1} - v_i)^3\\)\n\\(\\le\\frac{K}{12}\\sum_{i=0}^{n}(v_{i + 1} - v_i)^3\\)\nLet \\(x_i = v_{i+1} - v_i\\). It’s well known that these lengths are jointly distributed like a Dirichlet 3.\n3 See hereThis makes the \\((x_0, ... , x_{n}) \\sim Dirichlet(1, 1, ...., 1)\\)\nWe’ll use the following properties of moments of the Dirichlet distribution (n.d.).\n\nLin, Jiayu. n.d. “On the Dirichlet Distribution.” https://mast.queensu.ca/~communications/Papers/msc-jiayu-lin.pdf.\n\\(\\mathbb{E}[X_i^k] = \\frac{\\Gamma(n)}{\\Gamma(n + k)} \\frac{\\Gamma(k + 1)}{\\Gamma(1)} = \\frac{(n - 1)!k!}{(n + k - 1)!} = \\frac{k!}{n (n + 1) (n + 2) ... (n + k - 1)}\\) \\(\\mathbb{E}[X_i^kX_j^k] = \\frac{\\Gamma(n)}{\\Gamma(n + 2k)} \\frac{\\Gamma(k + 1)^2}{\\Gamma(1)} = \\frac{(n - 1)!(k!)^2}{(n + 2k - 1)!} = \\frac{k!^2}{n (n + 1) (n + 2) ... (n + 2k - 1)}\\)\nThat is the \\(k\\)-th marginal moment grows like \\(O(\\frac{1}{n^k})\\). and that the mixed moment \\(\\mathbb{E}[X_i^kX_j^k]\\) grows like \\(O(\\frac{1}{n^{2k}})\\).\nWe can look at \\(\\mathbb{E}[(S'_n - I)^2] = \\mathbb{E}[(\\frac{K}{12})^2 (\\sum_{i=0}^{n + 1}(x_i)^3)^2]\\) = \\(\\frac{K^2}{144}\\sum_{i, j} \\mathbb{E}[x_i^3x_j^3]\\)\nAs the sum over the \\((n + 2)^2\\) terms is over moments which are all \\(O(\\frac{1}{n^6})\\), this results in \\(\\mathbb{E}[(S'_n - I)^2)] = O(\\frac{1}{n^4})\\) (with an \\(O(\\frac{1}{n^2})\\) bound on the bias) 4. A precise bound can be derived by carrying through the constants from the Trapezoid Rule and the Dirichlet moments.\n4 One can easily get an error bound on the bias \\(|\\mathbb{E}[S'_n - I]|\\) without squaring the sum. Mainly looking at \\(\\mathbb{E}[(S'_n - I)^2]\\) gives us a stronger form of convergence, and one can still recover a bound on the bias."
  },
  {
    "objectID": "posts/quad_mc.html#examples",
    "href": "posts/quad_mc.html#examples",
    "title": "Trapezoid Rule Monte Carlo",
    "section": "Examples",
    "text": "Examples\nHere’s some example on a few different functions on \\([0, 1]\\)\n\n\\(f = \\frac{1}{1 + x^2}\\)\n\\(g = e^x\\sin(\\frac{x}{2})\\)\n\\(h = x^3 - x + x^2\\).\n\n\n\n\n\nTable 1: Absolute Error in estimating the integral of \\(f = \\frac{1}{1 + x^2}\\), \\(g = e^x \\sin(\\frac{x}{2})\\), \\(h = x^3 - x + x^2\\).\n\n\n\n\n\n\n\n\nIntegral\nMethod\nN=10\nN=50\nN=100\nN=200\nN=1000\n\n\n\n\n0.785398\nMonte Carlo\n0.026491\n0.019358\n0.008144\n0.002608\n0.001540\n\n\n0.785398\nTrapezoid Rule\n0.000393\n0.000010\n0.000038\n0.000009\n0.000000\n\n\n0.785398\nSimpson's 1/3 Rule\n0.000002\n0.000010\n0.000001\n0.000000\n0.000000\n\n\n\n\n\n\n\n\n\n\nIntegral\nMethod\nN=10\nN=50\nN=100\nN=200\nN=1000\n\n\n\n\n0.488364\nMonte Carlo\n0.146778\n0.003383\n0.026795\n0.034787\n0.009164\n\n\n0.488364\nTrapezoid Rule\n0.005188\n0.000636\n0.000118\n0.000022\n0.000001\n\n\n0.488364\nSimpson's 1/3 Rule\n0.000131\n0.000002\n0.000000\n0.000000\n0.000000\n\n\n\n\n\n\n\n\n\n\nIntegral\nMethod\nN=10\nN=50\nN=100\nN=200\nN=1000\n\n\n\n\n0.083333\nMonte Carlo\n0.178741\n0.057350\n0.007101\n0.011000\n0.000868\n\n\n0.083333\nTrapezoid Rule\n0.011265\n0.001467\n0.000173\n0.000051\n0.000002\n\n\n0.083333\nSimpson's 1/3 Rule\n0.004317\n0.000017\n0.000001\n0.000000\n0.000000\n\n\n\n\n\n\n\n\nWe can see that the error of the estimates from the Trapezoid Monte Carlo estimator seem to decrease very rapidly."
  },
  {
    "objectID": "posts/quad_mc.html#improving-the-estimator",
    "href": "posts/quad_mc.html#improving-the-estimator",
    "title": "Trapezoid Rule Monte Carlo",
    "section": "“Improving” the Estimator",
    "text": "“Improving” the Estimator\nThe above estimator can be extended to other Newton-Cotes-type estimators. A Newton-Cotes formula is an integration rule that estimates the integral via the integration of a polynomial interpolant on \\(k + 1\\) equally spaced points.\nFor instance, the trapezoid rule is just the integral of the linear polynomial between two equally spaced points.\nFor our purposes, we can take the interpolant and use it to derive an integration rule on non-qequally spaced points.\nA Simpson’s Rule-type estimator, for instance would be of order 2 and use the \\(3\\) points next to each other (as seen in Figure 2).\n\n\n\n\n\n\n\n\n\nFigure 2: Randomized Simpson Rule for \\(\\sin(2 \\pi x)\\)\n\n\n\n\nThe interpolating polynomial through the ordered points \\(a, b, c\\) would be \\(p(x) = f(a)\\frac{(c - x)(b - x)}{(c - a)(b - a)} + f(b)\\frac{(c - x)(a - x)}{(c - b)(a - c)} + f(c)\\frac{(a - x)(b - x)}{(a - c)(b - c)}\\).\nThen \\(\\int_a^c p(x) dx = -\\frac{(a - c)(f(b)(a - c)^2 + f(a)(c - b)(2(b - a) - (c - b)) + f(c)(b - a)(2(c - b) - (b - a)))}{6(b - a)(c - b)}\\)\nIf we call this result \\(g(a, b, c)\\), then we can form the estimator\n\\(S''_n = \\sum_{i=0}^{\\frac{n - 1}{2}} g(v_{2i}, v_{2i+1}, v_{2i+2})\\)\nError analysis for this can proceed in a similar way (though very messily), which should result in an estimator with \\(\\mathbb{E}[(S''_n - I)^2] = O(\\frac{1}{n^8})\\). Similary we should have a \\(O(\\frac{1}{n^4})\\) bound on the bias.\nIn general, we can set up estimators from larger interpolating polynomials, to get asymptotically faster converging estimators 56.\n5 Note that polynomial interpolation on equally spaced points can be very numerically unstable. These estimators defend against this by being on random points, but some care needs to be taken.6 Note that while the Dirichlet moments will decay faster in \\(n\\) for higher order interpolants, they also pick up larger constants.As seen in Table 1, we empirically get faster convergence with the Simpson’s-rule type estimator."
  },
  {
    "objectID": "posts/quad_mc.html#extending-to-other-probability-distributions",
    "href": "posts/quad_mc.html#extending-to-other-probability-distributions",
    "title": "Trapezoid Rule Monte Carlo",
    "section": "Extending to other probability distributions",
    "text": "Extending to other probability distributions\nSuppose now we have \\(\\int_{[a, b]} f(x) p(x) dx\\), where \\(p(x)\\) is some probability density on some interval \\([a, b]\\)\nThe Monte Carlo estimate would be \\(\\sum_{i=0}^{n-1} f(x_i)\\), where \\(x_i \\sim p(x)\\). We would like to use our trapezoid estimator here, where we now have samples from \\(p(x)\\) rather than uniform samples.\nWe can naively apply the estimator by computing order statistics of \\(x_i\\), called \\(z_i\\), appending \\(a\\) and \\(b\\) to the ends much like before.\nThis gives the estimator \\(S'_n = \\sum_{i=0}^n \\frac{1}{2}(z_{i+1} - z_i) (f(z_{i+1})p(z_{i+1}) - f(z_i)p(z_i))\\)\nLet \\(v_i = F(z_i)\\), where \\(F\\) is the CDF (we’ll assume this is completely differentiable). Then we have\n\\(S'_n = \\sum_{i=0}^n \\frac{1}{2}(z_{i+1} - z_i) (f(z_{i+1})p(z_{i+1}) - f(z_i)p(z_i)))\\)\n\\(= \\sum_{i=0}^n \\frac{1}{2}(F^{-1}(v_{i+1})- F^{-1}(v_i)) (f(F^{-1}(v_{i+1}))p(F^{-1}(v_{i+1})) - f(F^{-1}(v_i))p(F^{-1}(v_{i+1})))\\)\nVia an application of the Mean Value Theorem we have: \\(S'_n = \\sum_{i=0}^n \\frac{1}{2}F'^{-1}(s_i) (v_{i+1} - v_i)(f(F^{-1}(v_{i+1}))p(F^{-1}(v_{i+1})) - f(F^{-1}(v_i))p(F^{-1}(v_i)))\\)\nfor some \\(s_i \\in [v_i, v_{i+1}]\\)\nThis suggests that if \\(\\sup_{[0, 1]} |F'^{-1}(u)|  = \\sup_{[a, b]} \\frac{1}{p(x)} &lt; \\infty\\), then we can pick up another constant and proceed with our error analysis as before, and thus get a \\(O(\\frac{1}{n^4})\\) bound on \\(\\mathbb{E}[(I - S'_n)^2]\\). More careful treatment in something like Phillipe (1997) can loosen this restriction.\nWe can apply this rule to a Beta density \\(p(x)\\) of a \\(Beta(2, 3)\\) random variable, with our functions from Table 1 to estimate \\(\\int_0^1 f(x)p(x) dx\\).\n\n\n\n\nTable 2: Absolute Error in estimating the integral of \\(f = \\frac{1}{1 + x^2}\\), \\(g = e^x \\sin(\\frac{x}{2})\\), \\(h = x^3 - x + x^2\\) with respect to the Beta distribution.\n\n\n\n\n\n\n\n\nIntegral\nMethod\nN=10\nN=50\nN=100\nN=200\nN=1000\n\n\n\n\n0.849556\nMonte Carlo\n0.022431\n0.015612\n0.006972\n0.006267\n0.003704\n\n\n0.849556\nTrapezoid Rule\n0.071244\n0.004386\n0.001175\n0.003429\n0.000172\n\n\n0.849556\nSimpson's 1/3 Rule\n0.021684\n0.000002\n0.000070\n0.000004\n0.000000\n\n\n\n\n\n\n\n\n\n\nIntegral\nMethod\nN=10\nN=50\nN=100\nN=200\nN=1000\n\n\n\n\n0.331193\nMonte Carlo\n0.047922\n0.032177\n0.012697\n0.005025\n0.003999\n\n\n0.331193\nTrapezoid Rule\n0.000814\n0.001131\n0.005256\n0.001520\n0.000146\n\n\n0.331193\nSimpson's 1/3 Rule\n0.011204\n0.000972\n0.001880\n0.001769\n0.000013\n\n\n\n\n\n\n\n\n\n\nIntegral\nMethod\nN=10\nN=50\nN=100\nN=200\nN=1000\n\n\n\n\n-0.085714\nMonte Carlo\n0.061996\n0.014922\n0.000664\n0.006580\n0.001264\n\n\n-0.085714\nTrapezoid Rule\n0.009933\n0.001845\n0.000278\n0.000761\n0.000123\n\n\n-0.085714\nSimpson's 1/3 Rule\n0.015353\n0.000206\n0.001751\n0.001618\n0.000059\n\n\n\n\n\n\n\n\nAs seen in Table 2, we can still outperform the naive Monte-Carlo estimator.\nIf we only have access to an unnormalized density \\(h(x)\\), \\(\\frac{h(x)}{Z} = p(x)\\), an application of Slutsky’s Theorem gives us the following estimator (where we simultaneously estimate the normalizing constant \\(Z\\)):\n\\(S'_n = \\frac{\\sum_{i=0}^n \\frac{1}{2}(z_{i+1} - z_i) (f(z_{i+1})h(z_{i+1}) - f(z_i)h(z_i))}{\\sum_{i=0}^n \\frac{1}{2}(z_{i+1} - z_i) (h(z_{i+1}) - h(z_i))}\\)\nThus, given a non-zero unnormalized density on \\([a, b]\\) and access to a sampler, we can estimate an expectation to any smooth function with this estimator 7.\n7 In general we can mix and match estimators for the normalizing constant in the denominator. These estimators will be consistent, but in general will not be unbiased."
  },
  {
    "objectID": "posts/quad_mc.html#other",
    "href": "posts/quad_mc.html#other",
    "title": "Trapezoid Rule Monte Carlo",
    "section": "Other",
    "text": "Other\nThis type of estimator seems to have been originally derived from Yakowitz and Szidarovszky (1978) for the unit interval. Phillipe (1997) and related works extend the Riemann sum estimator to other distributions.\n\n\n\nYakowitz, Krimmel, S., and F. Szidarovszky. 1978. “Weighted Monte Carlo Integration.” SIAM Journal on Numerical Analysis 15 (6): 1289–1300.\n\nPhillipe, Anne. 1997. “Importance Sampling and Riemann Sums.” Publications IRMA Université de Lille 43 (IV)."
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Research",
    "section": "",
    "text": "Xingyou Song, Qiuyi Zhang, Chansoo Lee, Emily Fertig, Tzu-Kuo Huang, Lior Belinski, Greg Kochanski, Setareh Ariafar, Srinivas Vasudevan, Sagi Perel, Daniel Golovin\n                Aug 21, 2024\n                The Vizier Gaussian Process Bandit Algorithm\n                \n                \n                \n            \n            \n\n            \n                \n                    \n                        \n                    \n                        \n                        \n                        \n                             \n                             Arxiv\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Thomas Colthurst, Srinivas Vasudevan, James Lottes, Brian Patton\n                May 6, 2024\n                Fast Approximate Determinants Using Rational Functions\n                \n            \n            \n\n            \n                \n                    \n                        \n                    \n                        \n                        \n                        \n                             \n                             Arxiv\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Colin Carrol, Thomas Colthurst, Urs Koster, Srinivas Vasudevan\n                Mar 28, 2024\n                AutoBNN: Probabilistic time series forecasting with compositional bayesian neural networks\n                \n            \n            \n\n            \n                \n                    \n                        \n                    \n                        \n                        \n                        \n                             \n                             Google Research\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Erik Nijkamp, Ruiqi Gao, Pavel Sountsov, Srinivas Vasudevan, Bo Pang, Song-Chun Zhu, Ying Nian Wu\n                Jun 12, 2020\n                MCMC should mix: Learning energy-based model with neural transport latent space MCMC\n                \n            \n            \n\n            \n                \n                    \n                        \n                    \n                        \n                        \n                        \n                             \n                             Arxiv\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Pavel Sountsov, Alexy Radul, Srinivas Vasudevan\n                Jan 14, 2020\n                FunMC: A function API for building Markov Chains\n                \n            \n            \n\n            \n                \n                    \n                        \n                    \n                        \n                        \n                        \n                             \n                             Arxiv\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Matthew Hoffman, Pavel Sountsov, Joshua V Dillon, Ian Langmore, Dustin Tran, Srinivas Vasudevan\n                Mar 9, 2019\n                Neutra-lizing bad geometry in hamiltonian monte carlo using neural transport\n                \n            \n            \n\n            \n                \n                    \n                        \n                    \n                        \n                        \n                        \n                             \n                             Arxiv\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Dustin Tran, Matthew W Hoffman, Dave Moore, Christopher Suter, Srinivas Vasudevan, Alexey Radul\n                Nov 5, 2018\n                Simple, distributed, and accelerated probabilitic programming\n                \n            \n            \n\n            \n                \n                    \n                        \n                    \n                        \n                        \n                        \n                             \n                             Arxiv\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             \n                             Neurips'18\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Joshua V. Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore, Brian Patton, Alex Alemi, Matt Hoffman, Rif A Sauros\n                Nov 28, 2017\n                Tensorflow distributions\n                \n            \n            \n\n            \n                \n                    \n                        \n                    \n                        \n                        \n                        \n                             \n                             Arxiv\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\nNo matching items"
  },
  {
    "objectID": "boardgames/index.html",
    "href": "boardgames/index.html",
    "title": "Board Games",
    "section": "",
    "text": "Bitesize\n                    Designers: Srinivas Vasudevan\n                    Player Count: 2\n                  \n                  \n                \n            \n            \n            \n                \n                    \n                            domino\n                        \n                    \n                    \n                            climbing\n                        \n                    \n                    \n                            shedding\n                        \n                    \n            \n            \n\n            \n                \n                    \n                    \n                        \n                             BGG\n                        \n                    \n                    \n                \n            \n        \n        \n        \n            \n                \n                  \n                    Carvers Park\n                    Designers: DJ Kenel, Srinivas Vasudevan\n                    Player Count: 3-4\n                  \n                  \n                  \n                  \n                \n            \n            \n            \n                \n                    \n                            domino\n                        \n                    \n                    \n                            climbing\n                        \n                    \n                    \n                            shedding\n                        \n                    \n            \n            \n\n            \n                \n                    \n                    \n                        \n                             BGG\n                        \n                    \n                    \n                \n            \n        \n        \n        \n            \n                \n                  \n                    Lepidoptery\n                    Designers: David Karesh, Srinivas Vasudevan\n                    Player Count: 2\n                  \n                  \n                  \n                  \n                \n            \n            \n            \n                \n                    \n                            climbing\n                        \n                    \n                    \n                            shedding\n                        \n                    \n            \n            \n\n            \n                \n                    \n                    \n                        \n                             BGG\n                        \n                    \n                    \n                \n            \n        \n        \n        \n            \n                \n                  \n                    Pikel\n                    Designers: Srinivas Vasudevan\n                    Player Count: 2\n                  \n                  \n                \n            \n            \n            \n                \n                    \n                            domino\n                        \n                    \n                    \n                            climbing\n                        \n                    \n                    \n                            shedding\n                        \n                    \n            \n            \n\n            \n                \n                    \n                    \n                        \n                             BGG\n                        \n                    \n                    \n                \n            \n        \n        \n        \n            \n                \n                  \n                    Zipper\n                    Designers: David Karesh, Srinivas Vasudevan\n                    Player Count: 2\n                  \n                  \n                \n            \n            \n            \n                \n                    \n                            domino\n                        \n                    \n                    \n                            climbing\n                        \n                    \n                    \n                            shedding\n                        \n                    \n            \n            \n\n            \n                \n                    \n                    \n                        \n                             BGG\n                        \n                    \n                    \n                \n            \n        \n        \n\n\nNo matching items"
  }
]