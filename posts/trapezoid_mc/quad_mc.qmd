---
title: "Trapezoid Rule Monte Carlo"
description: "Mixing the Trapezoid Rule with Monte Carlo"
author: "Srinivas Vasudevan"
date: "09/19/2025"
categories:
  - monte-carlo
  - quadrature
format:
  html:
    grid:
      margin-width: 350px
bibliography: quad_mc.bib
reference-location: margin
citation-location: margin
---

I recently encountered a Monte Carlo estimator [@masry1990] that uses the trapezoid rule and wanted to
write out / generalize it.

Given a function $f$ (we'll assume $f$ is $C^\infty$ for simplicity, but some of these arguments can be changed depending on the smoothness) defined on the unit interval $[0, 1]$, we would like to compute the integral $I = \int_0^1 f(x) dx$.
Quadrature methods can easily deal with estimating these integrals [^2]

[^2]: Methods such as Clenshaw-Curtis or Gaussian Quadrature can do very well on smooth integrands with errors of  ${\displaystyle O([2N]^{-k}/k)}$, where $N$ is the number of quadrature points, and $k$ is the number of derivatives $f$ has.


Our plan will be to come up with a lower variance (but biased) estimator, and then generalize to computing integrals of the form $\int_A f(x) p(x) dx$, where $p(x)$ is a probability
density on some subset of $\mathbb{R}$. Finally, we'll be able to get an estimator even if we only have access to a sampler and the unnormalized log density (e.g. in the case of MCMC).

The typical Monte-Carlo estimator for this type of integral is $S_n = \frac{1}{n}\sum_{i=1}^n f(u_i)$, where $u_i \sim U(0, 1)$ are IID
uniform samples. This is an unbiased estimator for $I$ and has variance $var(S_n) = \frac{var(f)}{n}$, where $var(f)$ is the variance of
the random variable $f(U), U \sim U(0, 1)$. In particular the variance is inversely proportional to $n$.

## Trapezoid Rule Estimator

If we look at error estimates of various quadrature rules, we see that the errors get better dependency than $O(\frac{1}{\sqrt n})$ for estimating $I$.

::: {.column-margin}
We can see from the table that we can get errors $S_n - I$ that are at least $O(\frac{1}{n^2})$ for various
Newton-Cotes formula. From [wikipedia](https://en.wikipedia.org/wiki/Newton%E2%80%93Cotes_formulas).

| Name | Error Bound |
|---------:|:------:|
| Trapezoid Rule  | $-\frac{1}{12n^2}f''(\xi)$  |
| Simpson's $\frac{1}{3}$ rule Rule    | $-\frac{1}{90n^4}f^{(4)}(\xi)$  |
| Boole's Rule      | $-\frac{8}{945n^6}f^{(6)}(\xi)$  |

where $\xi$ lies in the interval of integration $[a, b]$.
:::

The main idea is we can exploit the structure of the trapezoid rule (and in general Newton-Cotes rules)
to get better scaling of the variance with respect to the number of samples $n$.


Given $n$ samples $u_i \sim U(0, 1)$, we can sort the samples to form the order statistics $v_i$. We'll also add the
points $0$ and $1$ to get $n + 1$ points (let $v_0 = 0$ and $v_{n + 1} = 1$).

We can then consider the trapezoid rule estimator:
$S'_n = \sum_{i=0}^{n} \frac{1}{2} (v_{i+1} - v_i)(f(v_{i+1}) + f(v_i))$

```{python}
#| label: fig-trap-margin
#| fig-cap: Randomized Trapezoid Rule for $\sin(\frac{\pi}{2} x)$
#| fig-width: 1
#| fig-height: 1
#| echo: false
#| column: margin
#| message: false
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(39372)
plt.style.use('../../assets/modified-tufte.mplstyle')

x = np.random.uniform(size=4)
x = np.concatenate([[0.], np.sort(x), [1]])
y = np.sin(np.pi * x / 2.)
x_true = np.linspace(0, 1, 100)
y_true = np.sin(np.pi * x_true / 2.)

def plot(curve_color, line_color):
  fig, ax = plt.subplots(figsize=(3, 3))
  fig.patch.set_alpha(0.)
  ax.set_alpha(0.)
  ax.set_facecolor('none')
  [t.set_color(line_color) for t in ax.xaxis.get_ticklines()]
  [t.set_color(line_color) for t in ax.xaxis.get_ticklabels()]
  [t.set_color(line_color) for t in ax.yaxis.get_ticklines()]
  [t.set_color(line_color) for t in ax.yaxis.get_ticklabels()]

  plt.plot(x_true, y_true, label='f', color=curve_color)
  for i in range(5):
    plt.plot([x[i], x[i]], [0, y[i]], color=line_color)
    plt.plot([x[i], x[i + 1]],[y[i], y[i + 1]], color=line_color)
    plt.plot([x[i + 1], x[i + 1]], [0, y[i + 1]], color=line_color)
  plt.legend(loc='lower right')
  plt.show()

plot(curve_color='#14BDEB', line_color='#DC7027')
```

The intuition is that for sufficiently smooth $f$, $S'_n$ should be a good estimator of the integral $I$ due to the use of the trapezoid rule.
Note that $S'_n$ may be biased, and indeed @fig-trap-margin shows an example of this, where the trapezoid rule approximates the integral well,
but will always be an underestimate due to the concavity of the integrand.
The hope is that we may trade off some bias for a reduction in variance.

As mentioned above, we can note the following deficiencies of this method, compared to quadrature rules and monte carlo rules.

* This can be a biased estimator.
* This is for $U(0, 1)$ random variables and in one dimension, for which there are better methods.
* In the current formulation, you have to evaluate the endpoints $0$ and $1$. This can be problematic for integrands
  (such as certain $Beta(\alpha, \beta)$ densities which go off to $\infty$ in the tails). We can actually get away with $n$ points without
  evaluating the $0$ and $1$ points. Adding the points just simplifies the analysis and reduces the error by a constant.


## Error bound on $S'_n$

Let's take a look at the error $S'_n - I$.

For simplicity we'll reuse a form of the Trapezoid Rule Error [^3].

[^3]: See [here](https://mathweb.ucsd.edu/~ebender/20B/77_Trap.pdf)

If we are estimating $\int_{v_i}^{v_{i+1}} f(x) dx$, then the error
of using $\frac{1}{2}(f(v_{i+1}) + f(v_{i}))(v_{i+1} - v_i)$ is bounded by $\frac{K_i(v_{i+1} - v_i)^3}{12}$, where
$K_i = \sup_{[v_i, v_{i+1}]} |f''(x)|$ (this can be derived through repeated applications of the Mean Value Theorem).


$|S'_n - I| = |-\int_0^1 f(x) dx + \sum_{i=0}^{n}\frac{1}{2}(v_{i+1} - v_i) (f(v_{i+1}) - f(v_i))|$

$=|\sum_{i=0}^{n}\frac{1}{2}(v_{i+1} - v_i) (f(v_{i+1}) + f(v_i)) - \int_{v_i}^{v_{i+1}} f(x) dx|$

$\le\sum_{i=0}^{n}|\frac{1}{2}(v_{i+1} - v_i) (f(v_{i+1}) + f(v_i)) - \int_{v_i}^{v_{i+1}} f(x) dx|$

$\le\sum_{i=0}^{n}\frac{K_i}{12}(v_{i + 1} - v_i)^3$

$\le\frac{K}{12}\sum_{i=0}^{n}(v_{i + 1} - v_i)^3$

Let $x_i = v_{i+1} - v_i$. It's well known that these lengths are jointly distributed like a Dirichlet [^4].

[^4]: See [here](https://statisticaloddsandends.wordpress.com/2021/04/20/generating-random-draws-from-the-dir1-1-1-distribution-using-the-uniform-distribution/)

This makes the $(x_0, ... , x_{n}) \sim Dirichlet(1, 1, ...., 1)$

We'll use the following properties of moments of the Dirichlet distribution [-@lin2016].


$\mathbb{E}[X_i^k] = \frac{\Gamma(n)}{\Gamma(n + k)} \frac{\Gamma(k + 1)}{\Gamma(1)} = \frac{(n - 1)!k!}{(n + k - 1)!} = \frac{k!}{n (n + 1) (n + 2) ... (n + k - 1)}$
$\mathbb{E}[X_i^kX_j^k] = \frac{\Gamma(n)}{\Gamma(n + 2k)} \frac{\Gamma(k + 1)^2}{\Gamma(1)} = \frac{(n - 1)!(k!)^2}{(n + 2k - 1)!} = \frac{k!^2}{n (n + 1) (n + 2) ... (n + 2k - 1)}$

That is the $k$-th marginal moment grows like $O(\frac{1}{n^k})$.
and that the mixed moment $\mathbb{E}[X_i^kX_j^k]$ grows like $O(\frac{1}{n^{2k}})$.


We can look at $\mathbb{E}[(S'_n - I)^2] = \mathbb{E}[(\frac{K}{12})^2 (\sum_{i=0}^{n + 1}(x_i)^3)^2]$ = $\frac{K^2}{144}\sum_{i, j} \mathbb{E}[x_i^3x_j^3]$

As the sum over the $(n + 2)^2$ terms is over moments which are all $O(\frac{1}{n^6})$, this results in 
$\mathbb{E}[(S'_n - I)^2)] = O(\frac{1}{n^4})$ (with an $O(\frac{1}{n^2})$ bound on the bias) [^5].
A precise bound can be derived by carrying through the constants from the Trapezoid Rule and the Dirichlet moments.

[^5]: One can easily get an error bound on the bias $|\mathbb{E}[S'_n - I]|$ without squaring the sum. Mainly looking at $\mathbb{E}[(S'_n - I)^2]$ gives us a stronger form of convergence, and one can still recover a bound on the bias.


## Examples

Here's some example on a few different functions on $[0, 1]$

* $f = \frac{1}{1 + x^2}$
* $g = e^x\sin(\frac{x}{2})$
* $h = x^3 - x + x^2$.

```{python}
#| label: tbl-error
#| echo: false
#| results: asis
#| tbl-format: none
#| tbl-cap: Absolute Error in estimating the integral of $f = \frac{1}{1 + x^2}$, $g = e^x \sin(\frac{x}{2})$, $h = x^3 - x + x^2$.
#| message: false
#| layout-nrow: 3
import numpy as np
import pandas as pd
import scipy
from IPython.display import display, Latex


np.random.seed(1927)

def simpsons_rule(f, n):
  # This actually uses one fewer point so that we can use the rule exactly. Otherwise padding would have to be done.
  u = np.random.uniform(size=(n-1))
  v = np.sort(u)
  v = np.concatenate([[0.], v, [1.]])
  a = v[::2][:-1]
  b = v[1::2]
  c = v[::2][1:]
  integrand = -(a - c) * (f(b)  * (a - c)**2 + f(a) * (c - b) * (3 * b - 2 * a - c) + f(c) * (b - a) * (2 * c - 3 * b + a)) / (6 * (b - a) * (c - b))
  return np.sum(integrand, axis=-1)

def trapezoid_rule(f, n):
  u = np.random.uniform(size=n)
  v = np.sort(u)
  v = np.concatenate([[0.], v, [1.]])
  return np.sum(0.5 * (f(v[:-1]) + f(v[1:])) * (v[1:] - v[:-1]), axis=-1)

def monte_carlo(f, n):
  # Use 2 more points so we use the same number of points.
  u = np.random.uniform(size=(n + 2))
  return np.mean(f(u))

def quadrature(f):
  return scipy.integrate.quad(f, 0., 1.)[0]

def f1(x):
  return 1 / (1 + x**2)

def f2(x):
  return np.exp(x) * np.sin(x/2.)

def f3(x):
  return x**3 - x + x**2


def render_table(f, name):
  integral = quadrature(f)
  data = {
    'Integral': [integral, integral, integral],
    'Method': ['Monte Carlo', 'Trapezoid Rule', "Simpson's 1/3 Rule"]
  }

  for grid_size in [8, 48, 98, 198, 998]:
    data[f'N={grid_size + 2}'] = [abs(integral - monte_carlo(f, grid_size))]
    data[f'N={grid_size + 2}'].append(abs(integral - trapezoid_rule(f, grid_size)))
    data[f'N={grid_size + 2}'].append(abs(integral - simpsons_rule(f, grid_size)))
  df = pd.DataFrame(data)
  display(df.style.hide(axis='index'))

render_table(f1, 'f')
render_table(f2, 'g')
render_table(f3, 'h')
```

We can see that the error of the estimates from the Trapezoid Monte Carlo estimator seem to decrease very rapidly.

## "Improving" the Estimator 
The above estimator can be extended to other Newton-Cotes-type estimators.
A Newton-Cotes formula is an integration rule that estimates the integral via the integration of a polynomial interpolant on $k + 1$ equally spaced points.

For instance, the trapezoid rule is just the integral of the linear polynomial between two equally spaced points.

For our purposes, we can take the interpolant and use it to derive an integration rule on non-qequally spaced points.

A Simpson's Rule-type estimator, for instance would be of order 2 and use the $3$ points next to each other (as seen in @fig-simpson-margin).

```{python}
#| label: fig-simpson-margin
#| fig-cap: Randomized Simpson Rule for $\sin(2 \pi x)$
#| fig-width: 0.001
#| fig-height: 0.001
#| echo: false
#| column: margin
#| message: false
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(2137)
plt.style.use('../../assets/modified-tufte.mplstyle')

x = np.random.uniform(size=5)
x = np.concatenate([[0.], np.sort(x), [1]])
y = np.sin(2 * np.pi * x)
x_true = np.linspace(0, 1, 100)
y_true = np.sin(2 * np.pi * x_true)

def quadratic(xs, ys):
  return np.poly1d(np.polyfit(xs, ys, 2))

def plot(curve_color, line_color):
  fig, ax = plt.subplots(figsize=(3, 3))
  fig.patch.set_alpha(0.)
  ax.set_alpha(0.)
  ax.set_facecolor('none')
  [t.set_color(line_color) for t in ax.xaxis.get_ticklines()]
  [t.set_color(line_color) for t in ax.xaxis.get_ticklabels()]
  [t.set_color(line_color) for t in ax.yaxis.get_ticklines()]
  [t.set_color(line_color) for t in ax.yaxis.get_ticklabels()]

  plt.plot(x_true, y_true, label='f', color=curve_color)
  # Draw polynomial interpolants
  for i in range(3):
    xs = x[(2 * i): (2 * i + 3)]
    ys = y[(2 * i): (2 * i + 3)]
    interpolant = quadratic(xs, ys)
    coords = np.linspace(xs[0], xs[1], 20)
    plt.plot(coords, interpolant(coords), color=line_color)
  plt.legend(loc='lower right')
  plt.show()

plot(curve_color='#14BDEB', line_color='#DC7027')
```

The interpolating polynomial through the ordered points $a, b, c$
would be $p(x) = f(a)\frac{(c - x)(b - x)}{(c - a)(b - a)} + f(b)\frac{(c - x)(a - x)}{(c - b)(a - c)} + f(c)\frac{(a - x)(b - x)}{(a - c)(b - c)}$.

Then $\int_a^c p(x) dx = -\frac{(a - c)(f(b)(a - c)^2 + f(a)(c - b)(2(b - a) - (c - b)) + f(c)(b - a)(2(c - b) - (b - a)))}{6(b - a)(c - b)}$

If we call this result $g(a, b, c)$, then we can form the estimator

$S''_n = \sum_{i=0}^{\frac{n - 1}{2}} g(v_{2i}, v_{2i+1}, v_{2i+2})$



Error analysis for this can proceed in a similar way (though very messily), which should result in an estimator with $\mathbb{E}[(S''_n - I)^2] = O(\frac{1}{n^8})$.
Similary we should have a $O(\frac{1}{n^4})$ bound on the bias.

In general, we can set up estimators from larger interpolating polynomials, to get asymptotically faster converging estimators [^6][^7].

[^6]: Note that polynomial interpolation on equally spaced points can be very numerically unstable. These estimators defend against this by being on random points, but some care needs to be taken.
[^7]: Note that while the Dirichlet moments will decay faster in $n$ for higher order interpolants, they also pick up larger constants.

As seen in @tbl-error, we empirically get faster convergence with the Simpson's-rule type estimator.

## Extending to other probability distributions

Suppose now we have $\int_{[a, b]} f(x) p(x) dx$, where $p(x)$ is some probability density on some interval $[a, b]$

The Monte Carlo estimate would be $\sum_{i=0}^{n-1} f(x_i)$, where $x_i \sim p(x)$. We would like to use our trapezoid
estimator here, where we now have samples from $p(x)$ rather than uniform samples.

We can naively apply the estimator by computing order statistics of $x_i$, called $z_i$, appending $a$ and $b$ to the ends much like before.

This gives the estimator  $S'_n = \sum_{i=0}^n \frac{1}{2}(z_{i+1} - z_i) (f(z_{i+1})p(z_{i+1}) - f(z_i)p(z_i))$

Let $v_i = F(z_i)$, where $F$ is the CDF (we'll assume this is completely differentiable). Then we have

$S'_n = \sum_{i=0}^n \frac{1}{2}(z_{i+1} - z_i) (f(z_{i+1})p(z_{i+1}) - f(z_i)p(z_i)))$

$= \sum_{i=0}^n \frac{1}{2}(F^{-1}(v_{i+1})- F^{-1}(v_i)) (f(F^{-1}(v_{i+1}))p(F^{-1}(v_{i+1})) - f(F^{-1}(v_i))p(F^{-1}(v_{i+1})))$

Via an application of the Mean Value Theorem we have:
$S'_n = \sum_{i=0}^n \frac{1}{2}F'^{-1}(s_i) (v_{i+1} - v_i)(f(F^{-1}(v_{i+1}))p(F^{-1}(v_{i+1})) - f(F^{-1}(v_i))p(F^{-1}(v_i)))$

for some $s_i \in [v_i, v_{i+1}]$

This suggests that if $\sup_{[0, 1]} |F'^{-1}(u)|  = \sup_{[a, b]} \frac{1}{p(x)} < \infty$, then we can pick up another constant and proceed with our error analysis as before, and thus
get a $O(\frac{1}{n^4})$ bound on $\mathbb{E}[(I - S'_n)^2]$. More careful treatment in something like @phillipe1997 can loosen this restriction.

We can apply this rule to a Beta density $p(x)$ of a $Beta(2, 3)$ random variable, with our functions from @tbl-error to estimate $\int_0^1 f(x)p(x) dx$.

```{python}
#| label: tbl-error-beta
#| echo: false
#| results: asis
#| tbl-format: none
#| tbl-cap: Absolute Error in estimating the integral of $f = \frac{1}{1 + x^2}$, $g = e^x \sin(\frac{x}{2})$, $h = x^3 - x + x^2$ with respect to the Beta distribution.
#| message: false
#| layout-nrow: 3
import numpy as np
import pandas as pd
import scipy
from IPython.display import display, Latex


np.random.seed(19127)
alpha = 2.
beta = 3.

def simpsons_rule(g, n):
  # This actually uses one fewer point so that we can use the rule exactly. Otherwise padding would have to be done.
  f = lambda x: g(x) * scipy.stats.beta.pdf(x, alpha, beta)
  u = scipy.stats.beta.rvs(alpha, beta, size=(n-1))
  v = np.sort(u)
  v = np.concatenate([[0.], v, [1.]])
  a = v[::2][:-1]
  b = v[1::2]
  c = v[::2][1:]
  integrand = -(a - c) * (f(b)  * (a - c)**2 + f(a) * (c - b) * (3 * b - 2 * a - c) + f(c) * (b - a) * (2 * c - 3 * b + a)) / (6 * (b - a) * (c - b))
  return np.sum(integrand, axis=-1)

def trapezoid_rule(g, n):
  f = lambda x: g(x) * scipy.stats.beta.pdf(x, alpha, beta)
  u = scipy.stats.beta.rvs(alpha, beta, size=n)
  v = np.sort(u)
  v = np.concatenate([[0.], v, [1.]])
  return np.sum(0.5 * (f(v[:-1]) + f(v[1:])) * (v[1:] - v[:-1]), axis=-1)

def monte_carlo(g, n):
  f = lambda x: g(x)
  # Use 2 more points so we use the same number of points.
  u = scipy.stats.beta.rvs(alpha, beta, size=(n + 2))
  return np.mean(f(u))

def quadrature(g):
  f = lambda x: g(x) * scipy.stats.beta.pdf(x, alpha, beta)
  return scipy.integrate.quad(f, 0., 1.)[0]

def f1(x):
  return 1 / (1 + x**2)

def f2(x):
  return np.exp(x) * np.sin(x/2.)

def f3(x):
  return x**3 - x + x**2


def render_table(f, name):
  integral = quadrature(f)
  data = {
    'Integral': [integral, integral, integral],
    'Method': ['Monte Carlo', 'Trapezoid Rule', "Simpson's 1/3 Rule"]
  }

  for grid_size in [8, 48, 98, 198, 998]:
    data[f'N={grid_size + 2}'] = [abs(integral - monte_carlo(f, grid_size))]
    data[f'N={grid_size + 2}'].append(abs(integral - trapezoid_rule(f, grid_size)))
    data[f'N={grid_size + 2}'].append(abs(integral - simpsons_rule(f, grid_size)))
  df = pd.DataFrame(data)
  display(df.style.hide(axis='index'))

render_table(f1, 'f')
render_table(f2, 'g')
render_table(f3, 'h')
```

As seen in @tbl-error-beta, we can still outperform the naive Monte-Carlo estimator.

If we only have access to an unnormalized density $h(x)$, $\frac{h(x)}{Z} = p(x)$, an application of [Slutsky's Theorem](https://en.wikipedia.org/wiki/Slutsky%27s_theorem)
gives us the following estimator (where we simultaneously estimate the normalizing constant $Z$):

$S'_n = \frac{\sum_{i=0}^n \frac{1}{2}(z_{i+1} - z_i) (f(z_{i+1})h(z_{i+1}) - f(z_i)h(z_i))}{\sum_{i=0}^n \frac{1}{2}(z_{i+1} - z_i) (h(z_{i+1}) - h(z_i))}$

Thus, given a non-zero unnormalized density on $[a, b]$ and access to a sampler, we can estimate an expectation to any smooth function with this estimator [^8].

[^8]: In general we can mix and match estimators for the normalizing constant in the denominator. These estimators will be consistent, but in general will not be unbiased.


## Other
This type of estimator seems to have been originally derived from @yakowitz1978 for the unit interval. @phillipe1997 and related works extend the Riemann
sum estimator to other distributions.
